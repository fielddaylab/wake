{
   "__serializerVersion": 2,
   "jobs": [
      {
         "id": "arctic-missing-whale",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
            {
               "id": "ROVScanner",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 2,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "findWhale",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            },
            {
               "id": "findTracker",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "foundDetritus",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "arctic-time-of-death",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "arctic-disappearing-act",
               "date": {
                  "added": 1.32953970272584E+17
               }
            },
            {
               "id": "arctic-time-of-death",
               "date": {
                  "added": 1.32953970272584E+17,
                  "deprecated": 1.3295916222049E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 5,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "getPopulations",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.3294107995602E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 3,
               "scaffoldingComplexity": 3
            },
            {
               "id": "getPops",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "arctic-whale-csi",
         "date": {
            "added": 1.32914936623526E+17,
            "deprecated": 1.33138730177673E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 0,
            "topicComplexity": 0
         },
         "tasks": [
            {
               "id": "scanAndTag",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "scanIceAlgae",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "displaced-reef",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 2,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "visitSiteO",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Travel",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "getScans",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "getCounts",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "makeModel",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "hide-n-seek",
         "date": {
            "added": 1.32914936623526E+17,
            "deprecated": 1.3295916222049E+17
         },
         "requiredJobs": [
            {
               "id": "turtle-danger2",
               "date": {
                  "added": 1.32953970272584E+17,
                  "deprecated": 1.3295916222049E+17
               }
            },
            {
               "id": "hide-n-seek",
               "date": {
                  "added": 1.32953970272584E+17,
                  "deprecated": 1.3295916222049E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "Flashlight",
               "date": {
                  "added": 1.32953898419312E+17,
                  "deprecated": 1.3295916222049E+17
               }
            },
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.32953898419312E+17,
                  "deprecated": 1.3295916222049E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 3,
            "modeling": 4,
            "argumentation": 1,
            "topicComplexity": 0
         },
         "tasks": [
            {
               "id": "jellyScan",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.3295916222049E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "jellyObservation",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.3295916222049E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "jellyRates",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.3295916222049E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "jellyPrediction",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.3295916222049E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.3295916222049E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "bayou-oxygen-tracking",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "bayou-shrimp-tastrophe",
               "date": {
                  "added": 1.32953970272584E+17,
                  "deprecated": 1.33111006512628E+17
               }
            },
            {
               "id": "bayou-shrimp-yields",
               "date": {
                  "added": 1.33111006512628E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ROVScanner",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "SyncModel",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "ObservationTank",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "WaterModeling",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.33111006512628E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 3,
            "modeling": 3,
            "argumentation": 4,
            "topicComplexity": 3
         },
         "tasks": [
            {
               "id": "getScans",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.33111006512628E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "measureOxygenUsage",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.33111006512628E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "createModel",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.33111006512628E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            },
            {
               "id": "scanAll",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "measureCritters",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "makeVisual",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Model",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "reef-decision",
         "date": {
            "added": 1.32914936623526E+17,
            "deprecated": 1.3295916222049E+17
         },
         "requiredJobs": [
            {
               "id": "turtle-danger2",
               "date": {
                  "added": 1.32953970272584E+17,
                  "deprecated": 1.3295916222049E+17
               }
            },
            {
               "id": "reef-decision",
               "date": {
                  "added": 1.32953970272584E+17,
                  "deprecated": 1.3295916222049E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "InterveneModel",
               "date": {
                  "added": 1.32953898419312E+17,
                  "deprecated": 1.3295916222049E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 4,
            "argumentation": 1,
            "topicComplexity": 0
         },
         "tasks": [
            {
               "id": "interventionDecision",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.3295916222049E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.3295916222049E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "bayou-save-our-shrimp",
         "date": {
            "added": 1.32914936623526E+17,
            "deprecated": 1.33111006512628E+17
         },
         "requiredJobs": [
            {
               "id": "bayou-save-our-shrimp",
               "date": {
                  "added": 1.32953970272584E+17,
                  "deprecated": 1.33111006512628E+17
               }
            }
         ],
         "requiredUpgrades": [
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 0,
            "topicComplexity": 0
         },
         "tasks": [
            {
               "id": "getScans",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.33111006512628E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.33111006512628E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "bayou-shrimp-tastrophe",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "bayou-save-our-shrimp",
               "date": {
                  "added": 1.32953970272584E+17,
                  "deprecated": 1.33111006512628E+17
               }
            },
            {
               "id": "bayou-dirty-detritus",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "bayou-oxygen-tracking",
               "date": {
                  "added": 1.33111006512628E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ROVScanner",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "SyncModel",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "ObservationTank",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.33311514440551E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 5,
            "modeling": 5,
            "argumentation": 5,
            "topicComplexity": 3
         },
         "tasks": [
            {
               "id": "measureOxygen",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.33111006512628E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 3,
               "scaffoldingComplexity": 3
            },
            {
               "id": "createModel",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Model",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "turtle-danger",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "displaced-reef",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ObservationTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 1,
            "modeling": 1,
            "argumentation": 4,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "experimentsForSiteO",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "visualModelO",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Model",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            },
            {
               "id": "scanCyano",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "countCyano",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "turtle-danger2",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "turtle-danger",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "PredictionModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVScanner",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "StressTank",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 4,
            "modeling": 3,
            "argumentation": 1,
            "topicComplexity": 3
         },
         "tasks": [
            {
               "id": "historicalData",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "syncModel",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Model",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            },
            {
               "id": "measureStress",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Experiment",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "coral-casting-shade",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "coral-stressed",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "StressTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "PropGuard",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "SyncModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVScanner",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 3,
            "modeling": 3,
            "argumentation": 1,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "scanSarg",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "popProbe",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "chemHistory",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "sargGrowthRate",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 1
            },
            {
               "id": "sargLight",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "coralGrowthRate",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "coralGrowthStressed",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "sargModel",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Model",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            },
            {
               "id": "sargArgue",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            },
            {
               "id": "coralLight",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "coral-eat-seaweed",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "coral-casting-shade",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
         ],
         "difficulties": {
            "experimentation": 2,
            "modeling": 0,
            "argumentation": 2,
            "topicComplexity": 0
         },
         "tasks": [
            {
               "id": "urchinSarg",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            },
            {
               "id": "urchinSargArgue",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "coral-fake-fix",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "coral-turtle-population",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 4,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "findReef",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Travel",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "scanProbes",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "countPopulation",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "argue",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "coral-fishy-bizz",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "InterveneModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 4,
            "argumentation": 3,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "getScans",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "historicalPopulations",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "getModel",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Model",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "coral-hunting-lions",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "coral-lionfish-conspiracy",
               "date": {
                  "added": 1.32953970272584E+17
               }
            },
            {
               "id": "coral-hunting-lions",
               "date": {
                  "added": 1.32953970272584E+17,
                  "deprecated": 1.3305916268445E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "PredictionModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "InterveneModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "StressTank",
               "date": {
                  "added": 1.33067919669553E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 5,
            "modeling": 5,
            "argumentation": 5,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "scanSpear",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.3305916268445E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "scanCritters",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "scanHistory",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "countPopulation",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "observeEatRules",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "obtainStressRules",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "measureStressEatRates",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.3305916268445E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "measureStressReproduceRates",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "obtainChemistry",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "predictModel",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Model",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "interveneModel",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Model",
               "taskComplexity": 3,
               "scaffoldingComplexity": 1
            },
            {
               "id": "argueIncentive",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 3,
               "scaffoldingComplexity": 3
            },
            {
               "id": "measureEatRates",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Experiment",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "coral-lionfish-conspiracy",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "coral-fake-fix",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ObservationTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 1,
            "modeling": 0,
            "argumentation": 3,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "scanNew",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "observeInteractions",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "arguePredator",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            },
            {
               "id": "observeCoral",
               "date": {
                  "added": 1.33080912981555E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "coral-much-algae",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "coral-fake-fix",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "SyncModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 3,
            "modeling": 3,
            "argumentation": 3,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "scanAll",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "observeEatAlgae",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "measureEatRate",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "modelPopulations",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Model",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "histPop",
               "date": {
                  "added": 1.32953898419312E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "coral-ocean-plastics",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "PropGuard",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 2,
            "modeling": 0,
            "argumentation": 1,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "scanReefEdge",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "whatTurtlesEat",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "makeModel",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "coral-stressed",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
            {
               "id": "StressTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "PropGuard",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 2,
            "modeling": 0,
            "argumentation": 4,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "siteR",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "stressCoral",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "coral-turtle-population",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 1,
            "topicComplexity": 0
         },
         "tasks": [
            {
               "id": "scanTurtle",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "turtleModel",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            },
            {
               "id": "countTurtle",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "coral-turtle-stability",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
            {
               "id": "Microscope",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 1,
            "modeling": 1,
            "argumentation": 1,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "scanAll",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "behavior",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            },
            {
               "id": "turtleModel",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Model",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "coral-urchin-friends",
         "date": {
            "added": 1.32914936623526E+17,
            "deprecated": 1.33138730177673E+17
         },
         "requiredJobs": [
            {
               "id": "coral-urchin-friends",
               "date": {
                  "added": 1.32953970272584E+17,
                  "deprecated": 1.33138730177673E+17
               }
            }
         ],
         "requiredUpgrades": [
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 0,
            "topicComplexity": 0
         },
         "tasks": [
            {
               "id": "coralUrchinModel",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.33138730177673E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "urchinStressed",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.33138730177673E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "siteRModel",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.33138730177673E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "present",
               "date": {
                  "added": 1.32914936623526E+17,
                  "deprecated": 1.33138730177673E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "kelp-bull-kelp-forest",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "kelp-welcome",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
         ],
         "difficulties": {
            "experimentation": 1,
            "modeling": 0,
            "argumentation": 1,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "siteA",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "foodweb",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "kelp-energy",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "kelp-shop-welcome",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 1,
            "argumentation": 1,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "foodweb",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Model",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "kelp-mussel-fest",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "kelp-welcome",
               "date": {
                  "added": 1.33111006512628E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "StressTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 2,
            "modeling": 0,
            "argumentation": 2,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "scanMussels",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "waterData",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "stressParam",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "report",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "kelp-musselfest-solution",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "kelp-mussel-fest",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "StressTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 2,
            "modeling": 0,
            "argumentation": 3,
            "topicComplexity": 3
         },
         "tasks": [
            {
               "id": "unstressedRate",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "stressedRate",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportChange",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "kelp-refuge-failure",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "kelp-start-refuge",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "PredictionModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 4,
            "modeling": 3,
            "argumentation": 3,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "visitSite",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Travel",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "histPop",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "growthRates",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 1
            },
            {
               "id": "growthRatesBull",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 1
            },
            {
               "id": "lightRatesGiant",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 1
            },
            {
               "id": "lightRatesBull",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 1
            },
            {
               "id": "newModel",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Model",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            },
            {
               "id": "getPaid",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 3
            },
            {
               "id": "histChem",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "kelp-refuge-failure-simulation",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "kelp-refuge-failure",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "PredictionModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 4,
            "argumentation": 1,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "createModel",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Model",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "shareModel",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "kelp-save-urchin-barren",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "kelp-urchin-barren-predict",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "InterveneModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 4,
            "argumentation": 2,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "makeAPlan",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Model",
               "taskComplexity": 3,
               "scaffoldingComplexity": 1
            },
            {
               "id": "return",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "kelp-start-refuge",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "kelp-welcome",
               "date": {
                  "added": 1.33111006512628E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "StressTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 2,
            "modeling": 0,
            "argumentation": 4,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "getBullKelpStress",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            },
            {
               "id": "getGiantKelpStress",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            },
            {
               "id": "argueSite",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "kelp-urchin-barren-predict",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "kelp-urchin-barren-viz",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "PredictionModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 3,
            "argumentation": 1,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "predictSiteB",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Model",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "kelp-urchin-barren-viz",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "kelp-welcome",
               "date": {
                  "added": 1.33111006512628E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 1,
            "argumentation": 1,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "gotoSiteB",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Travel",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "getProbeData",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "getTagged",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "visualSiteB",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Model",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "kelp-urchin-farm",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
            {
               "id": "kelp-welcome",
               "date": {
                  "added": 1.33111006512628E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 3,
            "modeling": 0,
            "argumentation": 4,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "urchinEatBull",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 1
            },
            {
               "id": "urchinEatGiant",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 1
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "kelp-welcome",
         "date": {
            "added": 1.32914936623526E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
         ],
         "difficulties": {
            "experimentation": 1,
            "modeling": 0,
            "argumentation": 1,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "gotoSiteC",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Travel",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "scanGiantKelp",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "scanUrchin",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "scanOtter",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "returnToShip",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Travel",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "runExperiment",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.32914936623526E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "arctic-cause-of-death",
         "date": {
            "added": 1.3294107995602E+17
         },
         "requiredJobs": [
            {
               "id": "arctic-disappearing-act",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "PredictionModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVScanner",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 1,
            "modeling": 0,
            "argumentation": 5,
            "topicComplexity": 3
         },
         "tasks": [
            {
               "id": "getEnvironmentData",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "getPopulationData",
               "date": {
                  "added": 1.3294107995602E+17,
                  "deprecated": 1.3305916268445E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Argue",
               "taskComplexity": 3,
               "scaffoldingComplexity": 3
            },
            {
               "id": "getHistPopulationData",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "getCurrPopulationData",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "arctic-disappearing-act",
         "date": {
            "added": 1.3294107995602E+17
         },
         "requiredJobs": [
            {
               "id": "arctic-missing-whale",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ObservationTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVScanner",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.33311514440551E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 2,
            "modeling": 2,
            "argumentation": 1,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "scanCritters",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "modelInteractions",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Model",
               "taskComplexity": 1,
               "scaffoldingComplexity": 3
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "arctic-isolated-instance",
         "date": {
            "added": 1.3294107995602E+17
         },
         "requiredJobs": [
            {
               "id": "arctic-stationary-survival",
               "date": {
                  "added": 1.32953970272584E+17
               }
            },
            {
               "id": "arctic-isolated-instance",
               "date": {
                  "added": 1.32953970272584E+17,
                  "deprecated": 1.3305916268445E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "StressTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "Flashlight",
               "date": {
                  "added": 1.32953898419312E+17,
                  "deprecated": 1.33138730940424E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 3,
            "modeling": 0,
            "argumentation": 5,
            "topicComplexity": 3
         },
         "tasks": [
            {
               "id": "scanSponge",
               "date": {
                  "added": 1.3294107995602E+17,
                  "deprecated": 1.33311514440551E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "scanAll",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "experimentPredatorZ",
               "date": {
                  "added": 1.3294107995602E+17,
                  "deprecated": 1.3305916268445E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "findPredator",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "experimentPredatorAll",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "countPopulations",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 1,
               "scaffoldingComplexity": 0
            },
            {
               "id": "determineStress",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Experiment",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "discussFindings",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Argue",
               "taskComplexity": 3,
               "scaffoldingComplexity": 1
            },
            {
               "id": "checkCrab",
               "date": {
                  "added": 1.3305916268445E+17,
                  "deprecated": 1.33067919669553E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "checkSeal",
               "date": {
                  "added": 1.3305916268445E+17,
                  "deprecated": 1.33067919669553E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "checkShark",
               "date": {
                  "added": 1.3305916268445E+17,
                  "deprecated": 1.33067919669553E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportFinal",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            },
            {
               "id": "checkPredator",
               "date": {
                  "added": 1.33067919669553E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "arctic-stationary-survival",
         "date": {
            "added": 1.3294107995602E+17
         },
         "requiredJobs": [
            {
               "id": "arctic-underneath",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ObservationTank",
               "date": {
                  "added": 1.32953898419312E+17
               }
            },
            {
               "id": "ROVScanner",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 1,
            "modeling": 2,
            "argumentation": 1,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "visualModel",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Model",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "report",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "arctic-underneath",
         "date": {
            "added": 1.3294107995602E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
            {
               "id": "Flashlight",
               "date": {
                  "added": 1.32953898419312E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 1,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "explore",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.3294107995602E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "kelp-shop-welcome",
         "date": {
            "added": 1.32953898419312E+17
         },
         "requiredJobs": [
            {
               "id": "kelp-bull-kelp-forest",
               "date": {
                  "added": 1.32953970272584E+17
               }
            }
         ],
         "requiredUpgrades": [
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 0,
            "topicComplexity": 0
         },
         "tasks": [
            {
               "id": "gotoShop",
               "date": {
                  "added": 1.32953898419312E+17,
                  "deprecated": 1.3305916268445E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "getTagger",
               "date": {
                  "added": 1.32953898419312E+17
               },
               "category": "Narrative",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "getVisualModel",
               "date": {
                  "added": 1.32953898419312E+17
               },
               "category": "Narrative",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "returnShip",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Travel",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "bayou-hide-n-seek",
         "date": {
            "added": 1.3295916222049E+17
         },
         "requiredJobs": [
            {
               "id": "turtle-danger2",
               "date": {
                  "added": 1.3295916222049E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "Flashlight",
               "date": {
                  "added": 1.3295916222049E+17
               }
            },
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.3295916222049E+17
               }
            },
            {
               "id": "PredictionModel",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 3,
            "modeling": 4,
            "argumentation": 3,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "jellyScan",
               "date": {
                  "added": 1.3295916222049E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "jellyObservation",
               "date": {
                  "added": 1.3295916222049E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "jellyRates",
               "date": {
                  "added": 1.3295916222049E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "jellyPrediction",
               "date": {
                  "added": 1.3295916222049E+17
               },
               "category": "Model",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.3295916222049E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "bayou-reef-decision",
         "date": {
            "added": 1.3295916222049E+17
         },
         "requiredJobs": [
            {
               "id": "turtle-danger2",
               "date": {
                  "added": 1.3295916222049E+17
               }
            },
            {
               "id": "bayou-hide-n-seek",
               "date": {
                  "added": 1.3295916222049E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "InterveneModel",
               "date": {
                  "added": 1.3295916222049E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 4,
            "argumentation": 1,
            "topicComplexity": 3
         },
         "tasks": [
            {
               "id": "interventionDecision",
               "date": {
                  "added": 1.3295916222049E+17
               },
               "category": "Model",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.3295916222049E+17
               },
               "category": "Argue",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "above-n-below",
         "date": {
            "added": 1.3305916268445E+17
         },
         "requiredJobs": [
            {
               "id": "arctic-seal-habbits",
               "date": {
                  "added": 1.3305916268445E+17,
                  "deprecated": 1.33080912981555E+17
               }
            },
            {
               "id": "arctic-seal-habitats",
               "date": {
                  "added": 1.33067919669553E+17
               }
            }
         ],
         "requiredUpgrades": [
         ],
         "difficulties": {
            "experimentation": 2,
            "modeling": 1,
            "argumentation": 1,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "scanNew",
               "date": {
                  "added": 1.33080912981555E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "countNew",
               "date": {
                  "added": 1.33080912981555E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "experimentInteractions",
               "date": {
                  "added": 1.33080912981555E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "createModel",
               "date": {
                  "added": 1.33080912981555E+17
               },
               "category": "Model",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.33080912981555E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "arctic-in-ice",
         "date": {
            "added": 1.3305916268445E+17
         },
         "requiredJobs": [
            {
               "id": "arctic-salmon-monitoring",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ObservationTank",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "ROVScanner",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.33067919669553E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 1,
            "modeling": 0,
            "argumentation": 2,
            "topicComplexity": 1
         },
         "tasks": [
            {
               "id": "scanAlgae",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "countAlgae",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "observeEat",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 3
            }
         ]
      },
      {
         "id": "arctic-picky-eaters",
         "date": {
            "added": 1.3305916268445E+17
         },
         "requiredJobs": [
            {
               "id": "arctic-in-ice",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 3,
            "modeling": 0,
            "argumentation": 4,
            "topicComplexity": 3
         },
         "tasks": [
            {
               "id": "obtainRateAlgae",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "obtainRateDiatoms",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "arctic-salmon-competition",
         "date": {
            "added": 1.3305916268445E+17
         },
         "requiredJobs": [
            {
               "id": "arctic-in-ice",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "SyncModel",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "ObservationTank",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.33080912981555E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 1,
            "modeling": 3,
            "argumentation": 4,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "observeCod",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Experiment",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "modelFish",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Model",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Argue",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "arctic-salmon-monitoring",
         "date": {
            "added": 1.3305916268445E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
            {
               "id": "ROVScanner",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 1,
            "topicComplexity": 0
         },
         "tasks": [
            {
               "id": "countSalmon",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "arctic-seal-habbits",
         "date": {
            "added": 1.3305916268445E+17,
            "deprecated": 1.33080912981555E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "Icebreaker",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 1,
            "topicComplexity": 0
         },
         "tasks": [
            {
               "id": "findRibbon",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "findHabitat",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "findMicro",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "findPop",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "report",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "bayou-alt-energy",
         "date": {
            "added": 1.3305916268445E+17
         },
         "requiredJobs": [
            {
               "id": "bayou-boom-cause",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ObservationTank",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "Hull",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 1,
            "modeling": 1,
            "argumentation": 3,
            "topicComplexity": 3
         },
         "tasks": [
            {
               "id": "scanCritters",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "wormEat",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "getModel",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Model",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "bayou-boom-cause",
         "date": {
            "added": 1.3305916268445E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
            {
               "id": "Flashlight",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "Hull",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 3,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "methaneScan",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "methaneTag",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "bayou-methanogen",
         "date": {
            "added": 1.3305916268445E+17
         },
         "requiredJobs": [
            {
               "id": "bayou-alt-energy",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "VisualModel",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "StressTank",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 4,
            "modeling": 0,
            "argumentation": 3,
            "topicComplexity": 3
         },
         "tasks": [
            {
               "id": "EatRule",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "StressRanges",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Experiment",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "WaterChem",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "Reproduction",
               "date": {
                  "added": 1.3305916268445E+17,
                  "deprecated": 1.33311514440551E+17
               },
               "category": "Experiment",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "Report",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            }
         ]
      },
      {
         "id": "bayou-test-model",
         "date": {
            "added": 1.3305916268445E+17,
            "deprecated": 1.33138730940424E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 0,
            "topicComplexity": 0
         },
         "tasks": [
            {
               "id": "obtainSyncModel",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "coral-tang-checkup",
         "date": {
            "added": 1.3305916268445E+17
         },
         "requiredJobs": [
            {
               "id": "coral-much-algae",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "Microscope",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "StressTank",
               "date": {
                  "added": 1.3305916268445E+17
               }
            },
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.3305916268445E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 3,
            "modeling": 3,
            "argumentation": 2,
            "topicComplexity": 3
         },
         "tasks": [
            {
               "id": "findStress",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Experiment",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "findMicro",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "countIck",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "observeIck",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reviseModels",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Model",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "report",
               "date": {
                  "added": 1.3305916268445E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "arctic-seal-habitats",
         "date": {
            "added": 1.33067919669553E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.33067919669553E+17
               }
            },
            {
               "id": "Icebreaker",
               "date": {
                  "added": 1.33067919669553E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.33067919669553E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 2,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "findHabitat",
               "date": {
                  "added": 1.33067919669553E+17
               },
               "category": "Travel",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "findRibbon",
               "date": {
                  "added": 1.33067919669553E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "findMicro",
               "date": {
                  "added": 1.33067919669553E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "countMicro",
               "date": {
                  "added": 1.33067919669553E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "findPop",
               "date": {
                  "added": 1.33067919669553E+17
               },
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "report",
               "date": {
                  "added": 1.33067919669553E+17
               },
               "category": "Argue",
               "taskComplexity": 1,
               "scaffoldingComplexity": 1
            },
            {
               "id": "countSeal",
               "date": {
                  "added": 1.33080912981555E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "arctic-endangered-seals",
         "date": {
            "added": 1.33080912981555E+17
         },
         "requiredJobs": [
            {
               "id": "above-n-below",
               "date": {
                  "added": 1.33080912981555E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ObservationTank",
               "date": {
                  "added": 1.33080912981555E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.33080912981555E+17
               }
            },
            {
               "id": "ROVScanner",
               "date": {
                  "added": 1.33080912981555E+17
               }
            },
            {
               "id": "PredictionModel",
               "date": {
                  "added": 1.33080912981555E+17
               }
            },
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.33080912981555E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.33080912981555E+17
               }
            },
            {
               "id": "ProbeHacker",
               "date": {
                  "added": 1.33080912981555E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 5,
            "modeling": 4,
            "argumentation": 4,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "scanProbe",
               "date": {
                  "added": 1.33080912981555E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "modelSync",
               "date": {
                  "added": 1.33080912981555E+17
               },
               "category": "Model",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "modelPredict",
               "date": {
                  "added": 1.33080912981555E+17
               },
               "category": "Model",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.33080912981555E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "bayou-blue-waters",
         "date": {
            "added": 1.33111006512628E+17
         },
         "requiredJobs": [
            {
               "id": "bayou-shrimp-yields",
               "date": {
                  "added": 1.33111006512628E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ROVScanner",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "Microscope",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "StressTank",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "MeasurementTank",
               "date": {
                  "added": 1.33111006512628E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 5,
            "modeling": 0,
            "argumentation": 1,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "findGreen",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "countGreen",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "stressCB",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Experiment",
               "taskComplexity": 2,
               "scaffoldingComplexity": 2
            },
            {
               "id": "measureEffect",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "measureReproduce",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "bayou-dirty-detritus",
         "date": {
            "added": 1.33111006512628E+17
         },
         "requiredJobs": [
            {
               "id": "bayou-blue-waters",
               "date": {
                  "added": 1.33111006512628E+17
               }
            }
         ],
         "requiredUpgrades": [
            {
               "id": "ROVScanner",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.33111006512628E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 3,
            "modeling": 0,
            "argumentation": 3,
            "topicComplexity": 2
         },
         "tasks": [
            {
               "id": "scanNew",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "countDetritus",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "eatDetritus",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Experiment",
               "taskComplexity": 1,
               "scaffoldingComplexity": 2
            },
            {
               "id": "growDetritus",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Experiment",
               "taskComplexity": 3,
               "scaffoldingComplexity": 2
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            },
            {
               "id": "scanProbe",
               "date": {
                  "added": 1.33311514440551E+17
               },
               "category": "Argue",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      },
      {
         "id": "bayou-shrimp-yields",
         "date": {
            "added": 1.33111006512628E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
            {
               "id": "ROVScanner",
               "date": {
                  "added": 1.33111006512628E+17
               }
            },
            {
               "id": "ROVTagger",
               "date": {
                  "added": 1.33111006512628E+17
               }
            }
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 0,
            "argumentation": 3,
            "topicComplexity": 0
         },
         "tasks": [
            {
               "id": "countShrimp",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Scan_Count",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.33111006512628E+17
               },
               "category": "Argue",
               "taskComplexity": 2,
               "scaffoldingComplexity": 1
            }
         ]
      },
      {
         "id": "final-final",
         "date": {
            "added": 1.33138730177673E+17
         },
         "requiredJobs": [
         ],
         "requiredUpgrades": [
         ],
         "difficulties": {
            "experimentation": 0,
            "modeling": 2,
            "argumentation": 4,
            "topicComplexity": 0
         },
         "tasks": [
            {
               "id": "tellMom",
               "date": {
                  "added": 1.33138730177673E+17
               },
               "category": "Narrative",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "createModel",
               "date": {
                  "added": 1.33138730177673E+17
               },
               "category": "Model",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "reportBack",
               "date": {
                  "added": 1.33138730177673E+17
               },
               "category": "Argue",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "sneakOut",
               "date": {
                  "added": 1.33138730177673E+17
               },
               "category": "Narrative",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "getUncleHelp",
               "date": {
                  "added": 1.33138730177673E+17
               },
               "category": "Narrative",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "checkShip",
               "date": {
                  "added": 1.33138730177673E+17
               },
               "category": "Narrative",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "returnToShop",
               "date": {
                  "added": 1.33138730177673E+17
               },
               "category": "Narrative",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            },
            {
               "id": "performRescue",
               "date": {
                  "added": 1.33174634552926E+17
               },
               "category": "Narrative",
               "taskComplexity": 0,
               "scaffoldingComplexity": 0
            }
         ]
      }
   ],
   "jobsSummary": {
      "experimentSummary": [
         18,
         10,
         8,
         9,
         3,
         4
      ],
      "modelingSummary": [
         27,
         6,
         3,
         8,
         6,
         2
      ],
      "argumentationSummary": [
         0,
         18,
         8,
         11,
         10,
         5
      ]
   }
}